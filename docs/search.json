{"Structs/HoneyPolicy.html#/s:7HoneyRL0A6PolicyVyACXe_tcACmcfc":{"name":"init(qValues:)","abstract":"<p>Undocumented</p>","parent_name":"HoneyPolicy"},"Structs/HoneyPolicy.html#/s:7HoneyRL6PolicyP9StateTypeQa":{"name":"StateType","parent_name":"HoneyPolicy"},"Structs/HoneyPolicy.html#/s:7HoneyRL6PolicyP10ActionTypeQa":{"name":"ActionType","parent_name":"HoneyPolicy"},"Structs/HoneyPolicy.html#/s:7HoneyRL0A6PolicyV7qValuesXevp":{"name":"qValues","abstract":"<p>Undocumented</p>","parent_name":"HoneyPolicy"},"Structs/HoneyCaptureAgent.html#/s:7HoneyRL0A12CaptureAgentV6policy7visitedAcA0A6PolicyV_ShySSGtcfc":{"name":"init(policy:visited:)","abstract":"<p>Undocumented</p>","parent_name":"HoneyCaptureAgent"},"Structs/HoneyCaptureAgent.html#/s:7HoneyRL5AgentP10PolicyTypeQa":{"name":"PolicyType","parent_name":"HoneyCaptureAgent"},"Structs/HoneyCaptureAgent.html#/s:7HoneyRL5AgentP6policy10PolicyTypeQzSgvp":{"name":"policy","parent_name":"HoneyCaptureAgent"},"Structs/HoneyCaptureAgent.html#/s:7HoneyRL5AgentP6update6policyx10PolicyTypeQz_tF":{"name":"update(policy:)","parent_name":"HoneyCaptureAgent"},"Structs/HoneyCaptureAgent.html#/s:7HoneyRL12CaptureAgentP7visitedShy10PolicyType_05StateG0QZGvp":{"name":"visited","parent_name":"HoneyCaptureAgent"},"Structs/HoneyCaptureAgent.html#/s:7HoneyRL0A12CaptureAgentV3add5stateACSS_tF":{"name":"add(state:)","abstract":"<p>Undocumented</p>","parent_name":"HoneyCaptureAgent"},"Structs/HoneyAgent.html#/s:7HoneyRL0A5AgentV6policyAcA0A6PolicyV_tcfc":{"name":"init(policy:)","abstract":"<p>Undocumented</p>","parent_name":"HoneyAgent"},"Structs/HoneyAgent.html#/s:7HoneyRL5AgentP6policy10PolicyTypeQzSgvp":{"name":"policy","parent_name":"HoneyAgent"},"Structs/HoneyAgent.html#/s:7HoneyRL5AgentP5visit5statex10PolicyType_05StateG0QZ_tF":{"name":"visit(state:)","parent_name":"HoneyAgent"},"Structs/HoneyAgent.html#/s:7HoneyRL5AgentP6update6policyx10PolicyTypeQz_tF":{"name":"update(policy:)","parent_name":"HoneyAgent"},"Structs/HoneyAgent.html#/s:7HoneyRL5AgentP6reward2s12s2Sf10PolicyType_05StateH0QZ_AItF":{"name":"reward(s1:s2:)","parent_name":"HoneyAgent"},"Structs/HoneyAgent.html#/s:7HoneyRL5AgentP10PolicyTypeQa":{"name":"PolicyType","parent_name":"HoneyAgent"},"Structs/HoneyAgent.html":{"name":"HoneyAgent","abstract":"<p>An agent that can be used for honeypots. It learns episodes or requests and responses, and learns how to respond based on SARSA learning and maximimizing interaction with the attacker.</p>"},"Structs/HoneyCaptureAgent.html":{"name":"HoneyCaptureAgent","abstract":"<p>An agent that can be used for honeypots. It learns episodes or requests and responses, and learns how to respond based on SARSA learning. It focuses on maximizing information capture then maximizing interaction.</p>"},"Structs/HoneyPolicy.html":{"name":"HoneyPolicy","abstract":"<p>A policy that can be used for honeypots to respond to requests</p>"},"Protocols/SARSAAgent.html#/s:7HoneyRL10SARSAAgentPAAE5learn8episodesxSaySay10PolicyType_05StateG0QZ_AF_06ActionG0QZtGG_tF":{"name":"learn(episodes:)","abstract":"<p>Learns a q policy given episodes using the SARSA algorithm with an empty initial policy, and alpha and gamma hyperparameters set to 0.5.</p>","parent_name":"SARSAAgent"},"Protocols/SARSAAgent.html#/s:7HoneyRL10SARSAAgentPAAE5learn8episodes5alpha5gammaxSaySay10PolicyType_05StateI0QZ_AH_06ActionI0QZtGG_S2ftF":{"name":"learn(episodes:alpha:gamma:)","abstract":"<p>Learns a q policy given episodes, an initial policy, and alpha and gamma parameters, using the SARSA algorithm</p>","parent_name":"SARSAAgent"},"Protocols/QPolicy.html#/s:7HoneyRL7QPolicyP7qValuesXevp":{"name":"qValues","abstract":"<p>Undocumented</p>","parent_name":"QPolicy"},"Protocols/QPolicy.html#/s:7HoneyRL7QPolicyPyxXe_tcxmcAaBRzlufc":{"name":"init(qValues:)","abstract":"<p>Initializes a new q policy with provided q-values</p>","parent_name":"QPolicy"},"Protocols/QPolicy.html#/s:7HoneyRL7QPolicyP8changing5state6action2tox9StateTypeQz_06ActionI0QzSftF":{"name":"changing(state:action:to:)","abstract":"<p>Initializes a new Q policy with provided Q-values</p>","parent_name":"QPolicy"},"Protocols/QPolicy.html#/s:7HoneyRL6PolicyP6choose3for10ActionTypeQzSg05StateG0Qz_tF":{"name":"choose(for:)","parent_name":"QPolicy"},"Protocols/Policy.html#/s:7HoneyRL6PolicyP9StateTypeQa":{"name":"StateType","abstract":"<p>The state type of the policy</p>","parent_name":"Policy"},"Protocols/Policy.html#/s:7HoneyRL6PolicyP10ActionTypeQa":{"name":"ActionType","abstract":"<p>The action type of the policy</p>","parent_name":"Policy"},"Protocols/Policy.html#/s:7HoneyRL6PolicyP6choose3for10ActionTypeQzSg05StateG0Qz_tF":{"name":"choose(for:)","abstract":"<p>Chooses an action given a state</p>","parent_name":"Policy"},"Protocols/CaptureAgent.html#/s:7HoneyRL12CaptureAgentP7visitedShy10PolicyType_05StateG0QZGvp":{"name":"visited","abstract":"<p>States that have been visited</p>","parent_name":"CaptureAgent"},"Protocols/CaptureAgent.html#/s:7HoneyRL12CaptureAgentP3add5statex10PolicyType_05StateH0QZ_tF":{"name":"add(state:)","abstract":"<p>Undocumented</p>","parent_name":"CaptureAgent"},"Protocols/CaptureAgent.html#/s:7HoneyRL12CaptureAgentPAAE6reward2s12s2Sf10PolicyType_05StateI0QZ_AItF":{"name":"reward(s1:s2:)","parent_name":"CaptureAgent"},"Protocols/CaptureAgent.html#/s:7HoneyRL12CaptureAgentPAAE5visit5statex10PolicyType_05StateH0QZ_tF":{"name":"visit(state:)","abstract":"<p>Visits state</p>","parent_name":"CaptureAgent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP10PolicyTypeQa":{"name":"PolicyType","abstract":"<p>The policy type the agent will learn</p>","parent_name":"Agent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP10RewardTypea":{"name":"RewardType","abstract":"<p>The reward type the agent uses</p>","parent_name":"Agent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP10ActionTypea":{"name":"ActionType","abstract":"<p>The action type the agent uses</p>","parent_name":"Agent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP9StateTypea":{"name":"StateType","abstract":"<p>The state type the agent uses</p>","parent_name":"Agent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP15StateActionPaira":{"name":"StateActionPair","abstract":"<p>A state and an action</p>","parent_name":"Agent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP7Episodea":{"name":"Episode","abstract":"<p>An array of state action pairs</p>","parent_name":"Agent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP6policy10PolicyTypeQzSgvp":{"name":"policy","abstract":"<p>The agent&rsquo;s learned policy</p>","parent_name":"Agent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP6reward2s12s2Sf10PolicyType_05StateH0QZ_AItF":{"name":"reward(s1:s2:)","abstract":"<p>Gets the reward for transitioning between two given states</p>","parent_name":"Agent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP5learn8episodesxSaySay10PolicyType_05StateG0QZ_AF_06ActionG0QZtGG_tF":{"name":"learn(episodes:)","abstract":"<p>Learns given episodes</p>","parent_name":"Agent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP6update6policyx10PolicyTypeQz_tF":{"name":"update(policy:)","abstract":"<p>Updates the agent&rsquo;s learned policy</p>","parent_name":"Agent"},"Protocols/Agent.html#/s:7HoneyRL5AgentP5visit5statex10PolicyType_05StateG0QZ_tF":{"name":"visit(state:)","abstract":"<p>Transitions the agent with visiting a state</p>","parent_name":"Agent"},"Protocols.html#/s:7HoneyRL6ActionP":{"name":"Action","abstract":"<p>A representation of what an agent performs</p>"},"Protocols/Agent.html":{"name":"Agent","abstract":"<p>An agent learns a (possibly rewarding) policy given training data (episodes) and a reward function</p>"},"Protocols/CaptureAgent.html":{"name":"CaptureAgent","abstract":"<p>An agent that can be used for honeypots. It learns episodes or requests and responses, and learns how to respond based on SARSA learning and maximimizing information capture from the attackers. It essentially rewards transitioning into an unvisited state much more than transitioning into a visited state.</p>"},"Protocols/Policy.html":{"name":"Policy","abstract":"<p>A policy maps a state to an action.</p>"},"Protocols/QPolicy.html":{"name":"QPolicy","abstract":"<p>A policy that determined by Q values - which intend to measure the quality of any given state in respect to the reward received</p>"},"Protocols/SARSAAgent.html":{"name":"SARSAAgent","abstract":"<p>An agent that uses the SARSA algorithm (state–action–reward–state–action)</p>"},"Protocols.html#/s:7HoneyRL5StateP":{"name":"State","abstract":"<p>A representation of the current situation</p>"},"Extensions/Mapping.html#/s:7HoneyRL24transformToProbabilitiesyXeycXeF":{"name":"transformToProbabilities()","abstract":"<p>Transforms a vector of scalar values into a vector of probabilities (which should total 1)</p>","parent_name":"Mapping"},"Extensions/Mapping.html":{"name":"Mapping"},"Extensions.html":{"name":"Extensions","abstract":"<p>The following extensions are available globally.</p>"},"Protocols.html":{"name":"Protocols","abstract":"<p>The following protocols are available globally.</p>"},"Structs.html":{"name":"Structures","abstract":"<p>The following structures are available globally.</p>"}}
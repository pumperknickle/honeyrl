import Foundation
import AwesomeDictionary

/// An agent that uses the SARSA algorithm (state–action–reward–state–action)
public protocol SARSAAgent: Agent where PolicyType: QPolicy { }

public extension SARSAAgent {
    /**
    Learns a q policy given episodes using the SARSA algorithm with an empty initial policy, and alpha and gamma hyperparameters set to 0.5.

    - Parameter episodes: Training dataset of episodes

    - Returns: A learned policy based on q values generated by the SARSA algorithm
    */
    func learn(episodes: [Episode]) -> PolicyType {
        return learn(initialPolicy: PolicyType(qValues: Mapping<StateType, Mapping<ActionType, Float>>()), episodes: episodes, alpha: 0.5, gamma: 0.5)
    }
    
    /**
    Learns a q policy given episodes, an initial policy, and alpha and gamma parameters, using the SARSA algorithm

    - Parameters:
     - initialPolicy: The starting policy that will be modified
     - episodes: Training dataset of episodes
     - alpha: The learning rate determines to what extent newly acquired information overrides old information. A factor of 0 will make the agent not learn anything, while a factor of 1 would make the agent consider only the most recent information.
     - gamma: The discount factor determines the importance of future rewards. A factor of 0 makes the agent "opportunistic" by only considering current rewards, while a factor approaching 1 will make it strive for a long-term high reward. If the discount factor meets or exceeds 1, the Q values may diverge.

    - Returns: A learned policy based on Q values generated by the SARSA algorithm
    */
    func learn(initialPolicy: PolicyType, episodes: [Episode], alpha: Float, gamma: Float) -> PolicyType {
        return episodes.reduce(initialPolicy) { (result, entry) -> PolicyType in
            return learn(initialPolicy: result, episode: entry, alpha: alpha, gamma: gamma)
        }
    }
}

extension SARSAAgent {
    func learn(initialPolicy: PolicyType, episode: Episode, alpha: Float, gamma: Float) -> PolicyType {
        guard let firstStateActionPair = episode.first else { return initialPolicy }
        return episode.dropFirst().reduce((initialPolicy, firstStateActionPair)) { (result, entry) -> (PolicyType, StateActionPair) in
            return (learn(initialPolicy: result.0, sa1: result.1, sa2: entry, alpha: alpha, gamma: gamma), entry)
        }.0
    }
    
    func learn(initialPolicy: PolicyType, sa1: StateActionPair, sa2: StateActionPair, alpha: Float, gamma: Float) -> PolicyType {
        let r = reward(s1: sa1.0, s2: sa2.0)
        let q1 = initialPolicy.qValue(for: sa1.0, action: sa1.1) ?? 0.0
        let q2 = initialPolicy.qValue(for: sa2.0, action: sa2.1) ?? 0.0
        let newQ = q1 + (alpha * (r + (gamma * q2) - q1))
        return initialPolicy.changing(state: sa1.0, action: sa1.1, to: newQ)
    }
}
